# ü§ñ Projeto de Aprendizado por Refor√ßo Profundo

Este reposit√≥rio foi desenvolvido como parte da entrega do **Projeto 2 da disciplina de Rob√¥s M√≥veis Aut√¥nomos**, pelos alunos **Felipe Pereira Furlaneto** e **Marcos Vinicios dos Santos**.


## üìå Descri√ß√£o do Projeto

Este reposit√≥rio apresenta um estudo sobre Aprendizagem por Refor√ßo Profundo (Deep Reinforcement Learning ‚Äì DRL), com √™nfase na aplica√ß√£o do algoritmo Deep Q-Network (DQN) em um cen√°rio de navega√ß√£o aut√¥noma. A aprendizagem por refor√ßo profundo permite que agentes tomem decis√µes baseadas em intera√ß√µes com o ambiente, utilizando redes neurais profundas para aproximar fun√ß√µes de valor e lidar com espa√ßos de estados de alta dimensionalidade.

Um dos principais desafios em DRL est√° na defini√ß√£o adequada dos hiperpar√¢metros, que exercem influ√™ncia direta na efici√™ncia, estabilidade e converg√™ncia do processo de aprendizado. Dentre eles, o par√¢metro epsilon, respons√°vel por controlar o equil√≠brio entre explora√ß√£o (explore) e explora√ß√£o do conhecimento j√° adquirido (exploit) na estrat√©gia Œµ-greedy, possui papel central. Varia√ß√µes em sua taxa de decaimento ou valor inicial podem afetar de forma significativa o desempenho final do agente.

O objetivo deste projeto √© investigar a influ√™ncia do par√¢metro epsilon durante o treinamento de um agente DQN, bem como sua aplica√ß√£o em uma tarefa simulada de navega√ß√£o aut√¥noma. A proposta busca contribuir para uma compreens√£o mais aprofundada do impacto desse hiperpar√¢metro no processo de aprendizado e na tomada de decis√£o do agente.

---

## ü§ñ Descri√ß√£o do Rob√¥
O rob√¥ simulado neste projeto foi desenvolvido a partir de um modelo originalmente disponibilizado nos arquivos da disciplina de RMA 2025 (Robos M√≥veis Aut√¥nomos). Esse modelo foi exportado do SolidWorks para o formato URDF e adaptado para testes em ambientes virtuais no simulador Gazebo. Trata-se de uma plataforma com base circular e duas rodas laterais acionadas por um sistema de locomo√ß√£o diferencial.

A estrutura do rob√¥ inclui ainda um LIDAR 3D Velodyne HDL-32E, com 32 feixes de laser e resolu√ß√£o configur√°vel, ideal para mapeamento e detec√ß√£o de obst√°culos em 360¬∞

Essa estrutura pode ser observada abaixo:

<!-- Insira aqui uma imagem do rob√¥ -->
![Imagem do Rob√¥](imagem/robo.png)

---

## üåê Ambiente Simulado
O ambiente virtual utilizado nos experimentos foi desenvolvido no simulador Gazebo 11 e representa um cen√°rio interno composto por m√∫ltiplos obst√°culos e paredes delimitadoras, conforme ilustrado na imagem abaixo. A superf√≠cie principal consiste em um terreno plano com textura de gramado (ground_plane), sobre o qual foi constru√≠do um espa√ßo com paredes verticais que formam corredores e compartimentos interligados, simulando uma planta arquitet√¥nica simples com dimens√µes de 15 metros de largura por 15 metros de comprimento.

Dentro do ambiente, foram posicionados obst√°culos c√∫bicos pretos em diferentes locais, com o objetivo de aumentar a complexidade da navega√ß√£o e demandar do agente aut√¥nomo habilidades de desvio e planejamento de trajet√≥ria at√© o ponto final, destacado pelo quadro azul. O layout foi projetado para incluir √°reas estreitas, bifurca√ß√µes, espa√ßos abertos e uma zona de chegada, desafiando tanto a estrat√©gia de explora√ß√£o quanto a pol√≠tica de decis√£o aprendida pelo agente.

<!-- Insira aqui uma imagem do ambiente no Gazebo -->
![Ambiente no Gazebo](imagem/ambiente.png)

---

## ‚öôÔ∏è Instala√ß√£o e Execu√ß√£o

### üì• Clonando o Reposit√≥rio


```bash
cd  seu_workspace/src
git clone https://github.com/FelipePF22/Deep-Qlearn-AutoDrive.git
catkin build
source devel/setup.bash
```

### ‚ö†Ô∏è Aviso: Nesse projeto o nome do workspace esta como ros_ws, portanto antes de executar os launchs verifique o c√≥digo em python do respectivo launch para alterar o endere√ßo onde ser√° salvo os logs e a rede treinada

---

## üñ•Ô∏è Launch files

---

## üîÑ Treinamento do sistema: 

```bash
roslaunch q_learning_control q_learning_sim.launch
```

---

### ‚ö†Ô∏è Aviso: O reposit√≥rio contem tr√™s treinamentos para tr√™s √©psilons, dessa forma, afim de validar corretamente o treinamento, basta substituir o arquivo q_net.pth da pasta "network" pelo q_net.pth do respectivo resultado que deseja, ou seja, √©psilon = 1  ou √©psilon = 0.6 

### üî¥ Nota: Originalmente o arquivo network j√° contem o q_net.pth do √©psilon = 1.0
---

## üîç Valida√ß√£o do treinamento: 
```bash
roslaunch q_learning_control q_learning_validation.launch
```
---



