# ğŸ¤– Projeto de Aprendizado por ReforÃ§o Profundo

Este repositÃ³rio foi desenvolvido como parte da entrega do **Projeto 2 da disciplina de RobÃ´s MÃ³veis AutÃ´nomos**, pelos alunos **Felipe Pereira Furlaneto** e **Marcos Vinicios dos Santos**.


## ğŸ“Œ DescriÃ§Ã£o do Projeto

Este repositÃ³rio apresenta um estudo sobre Aprendizagem por ReforÃ§o Profundo (Deep Reinforcement Learning â€“ DRL), com Ãªnfase na aplicaÃ§Ã£o do algoritmo Deep Q-Network (DQN) em um cenÃ¡rio de navegaÃ§Ã£o autÃ´noma. A aprendizagem por reforÃ§o profundo permite que agentes tomem decisÃµes baseadas em interaÃ§Ãµes com o ambiente, utilizando redes neurais profundas para aproximar funÃ§Ãµes de valor e lidar com espaÃ§os de estados de alta dimensionalidade.

Um dos principais desafios em DRL estÃ¡ na definiÃ§Ã£o adequada dos hiperparÃ¢metros, que exercem influÃªncia direta na eficiÃªncia, estabilidade e convergÃªncia do processo de aprendizado. Dentre eles, o parÃ¢metro epsilon, responsÃ¡vel por controlar o equilÃ­brio entre exploraÃ§Ã£o (explore) e exploraÃ§Ã£o do conhecimento jÃ¡ adquirido (exploit) na estratÃ©gia Îµ-greedy, possui papel central. VariaÃ§Ãµes em sua taxa de decaimento ou valor inicial podem afetar de forma significativa o desempenho final do agente.

O objetivo deste projeto Ã© investigar a influÃªncia do parÃ¢metro epsilon durante o treinamento de um agente DQN, bem como sua aplicaÃ§Ã£o em uma tarefa simulada de navegaÃ§Ã£o autÃ´noma. A proposta busca contribuir para uma compreensÃ£o mais aprofundada do impacto desse hiperparÃ¢metro no processo de aprendizado e na tomada de decisÃ£o do agente.

---

## ğŸ¤– DescriÃ§Ã£o do RobÃ´
O robÃ´ simulado neste projeto foi desenvolvido a partir de um modelo originalmente disponibilizado nos arquivos da disciplina de RMA 2025 (Robos MÃ³veis AutÃ´nomos). Esse modelo foi exportado do SolidWorks para o formato URDF e adaptado para testes em ambientes virtuais no simulador Gazebo. Trata-se de uma plataforma com base circular e duas rodas laterais acionadas por um sistema de locomoÃ§Ã£o diferencial.

A estrutura do robÃ´ inclui ainda um LIDAR 3D Velodyne HDL-32E, com 32 feixes de laser e resoluÃ§Ã£o configurÃ¡vel, ideal para mapeamento e detecÃ§Ã£o de obstÃ¡culos em 360Â°

Essa estrutura pode ser observada abaixo:

<!-- Insira aqui uma imagem do robÃ´ -->
![Imagem do RobÃ´](imagem/robo.png)

---

## ğŸŒ Ambiente Simulado
O ambiente virtual utilizado nos experimentos foi desenvolvido no simulador Gazebo 11 e representa um cenÃ¡rio interno composto por mÃºltiplos obstÃ¡culos e paredes delimitadoras, conforme ilustrado na imagem abaixo. A superfÃ­cie principal consiste em um terreno plano com textura de gramado (ground_plane), sobre o qual foi construÃ­do um espaÃ§o com paredes verticais que formam corredores e compartimentos interligados, simulando uma planta arquitetÃ´nica simples com dimensÃµes de 15 metros de largura por 15 metros de comprimento.

Dentro do ambiente, foram posicionados obstÃ¡culos cÃºbicos pretos em diferentes locais, com o objetivo de aumentar a complexidade da navegaÃ§Ã£o e demandar do agente autÃ´nomo habilidades de desvio e planejamento de trajetÃ³ria atÃ© o ponto final, destacado pelo quadro azul. O layout foi projetado para incluir Ã¡reas estreitas, bifurcaÃ§Ãµes, espaÃ§os abertos e uma zona de chegada, desafiando tanto a estratÃ©gia de exploraÃ§Ã£o quanto a polÃ­tica de decisÃ£o aprendida pelo agente.

<!-- Insira aqui uma imagem do ambiente no Gazebo -->
![Ambiente no Gazebo](imagem/ambiente.png)

---

## âš™ï¸ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### ğŸ“¥ Clonando o RepositÃ³rio


```bash
cd  seu_workspace/src
git clone https://github.com/FelipePF22/Deep-Qlearn-AutoDrive.git
catkin build
source devel/setup.bash
```

### âš ï¸ Aviso: Nesse projeto o nome do workspace esta como ros_ws, portanto antes de executar os launchs verifique o cÃ³digo em python do respectivo launch para alterar o endereÃ§o onde serÃ¡ salvo os logs e a rede treinada

---

## ğŸ–¥ï¸ Launch files

---

## ğŸ”„ Treinamento do sistema: 

```bash
roslaunch q_learning_control q_learning_sim.launch
```

---

### âš ï¸ Aviso: O repositÃ³rio contem trÃªs treinamentos para trÃªs Ã©psilons, dessa forma, afim de validar corretamente o treinamento, basta substituir o arquivo q_net.pth da pasta "network" pelo q_net.pth do respectivo resultado que deseja, ou seja, Ã©psilon = 1 ou Ã©psilon = 0.8 ou Ã©psilon = 0.6 

### ğŸ”´ Nota: Originalmente o arquivo network jÃ¡ contem o q_net.pth do Ã©psilon = 1.0
---

## ğŸ” ValidaÃ§Ã£o do treinamento: 
```bash
roslaunch q_learning_control q_learning_validation.launch
```
---

## ğŸğŸ“ DepuraÃ§Ã£o da TrajetÃ³ria

O repositÃ³rio ainda contÃªm um arquivo denominado "trajectories", nele contem todas as trajetÃ³rias salvas em .jason realizadas pelo robÃ´ durante a execuÃ§Ã£o do "q_learning_sim.launch". Assim para a sua depuraÃ§Ã£o:

```bash
cd seu_workspace/src/DQlearn-AutoDrive/Autodriver/q_learning_control/scripts
python3 trajectorie.py 
```
ApÃ³s a sua excuÃ§Ã£o na pasta "caminhos" presente dentro de "trajectories" irÃ¡ exibir todas as trajetÃ³rias individuais.


